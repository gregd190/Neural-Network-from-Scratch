{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network from Scratch\n",
    "In this exercise, I will build a very simple neural network from scratch (not using any machine learning libraries). \n",
    "It is the good way to check my understanding of the underlying principles. \n",
    "\n",
    "The network will be tested on a very simply task - Adding 5 numbers together. This task was chosen because we can rapidly generate training data for it, and it should be reasonably solvable by a small network in a short period of time.\n",
    "\n",
    "The network will work on stochastic gradient descent, although it could easily be modified to batch learning.\n",
    "\n",
    "Firstly, we'll import NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create arrays to store the weights and biases. This will be a neural network with 2 hidden layers. That is, it will have an input layer, 2 hidden layers and an output layer. Values are randomly initialised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create arrays for weights and biases for each layer, randomly initialized. \n",
    "def create_network(input_size, L1_size, L2_size, output_size):\n",
    "    W1 = np.random.rand(L1_size, input_size)\n",
    "    W2 = np.random.rand(L2_size, L1_size)\n",
    "    W3 = np.random.rand(output_size, L2_size)\n",
    "    b1 = np.random.rand(L1_size)\n",
    "    b2 = np.random.rand(L2_size)\n",
    "    b3 = np.random.rand(output_size)\n",
    "    \n",
    "    return(W1,b1,W2,b2,W3,b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write a function to step forward through the network, calculating the output of a given layer given the outputs of the prior layer (or in the input layer itself). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes the output of the prior layer (or the initial inputs) and returns the output of the subsequent layer\n",
    "def forward_step(input_array, W, b):\n",
    "    output = np.zeros((len(W)))\n",
    "    for x in range(len(W)):\n",
    "        for y in range(len(input_array)):\n",
    "            output[x] += input_array[y]*W[x][y]\n",
    "        output[x] += b[x]\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between each layer, we will apply an activation function. We will use the relu function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes an input array and returns the relu version. \n",
    "def relu(input_array):\n",
    "    output_array = []\n",
    "    for a in input_array:\n",
    "        output_array.append(max(0,a))\n",
    "    return(output_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put together the forward-step and relu functions to run through our network and get the final output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combines the forward_step and relu functions to forward_step through the entire network\n",
    "def forward_propagate(input_array, W1,b1,W2,b2,W3,b3):\n",
    "    output_L1 = forward_step(input_array, W1, b1)\n",
    "    rect_output_L1 = relu(output_L1)\n",
    "    output_L2 = forward_step(rect_output_L1, W2, b2)\n",
    "    rect_output_L2 = relu(output_L2)\n",
    "    output_final = forward_step(rect_output_L2, W3, b3)\n",
    "    # ~ processed_output_final = output_final\n",
    "    return(output_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a 'cost function', a way to determine how accurate our network is. We will use the mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate mean square error loss:\n",
    "def calculate_mse_loss(prediction, answer):\n",
    "    loss = 0\n",
    "    for i in range(len(prediction)):\n",
    "        loss += (prediction[i]-answer[i])**2\n",
    "    return(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now would be a good time to test run a piece of training data through our network and calculate the loss. To do so we will need to generate some training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate training data. To start with, we are going to train our network to sum 5 integers.     \n",
    "def generate_training_data(num):\n",
    "    input_array = []\n",
    "    answer_array = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        inputs = np.random.randint(0,10,5)\n",
    "        answer = np.sum(inputs)\n",
    "        input_array.append(inputs)\n",
    "        answer_array.append([answer]) \n",
    "        \n",
    "    return(input_array, answer_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform one run through the network and verify that it is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input =  [array([1, 9, 9, 9, 3])] Train_output =  [[31]] Prediction =  [ 88.02669107] Loss =  [ 3252.04349401]\n"
     ]
    }
   ],
   "source": [
    "#Generate Training Data\n",
    "train_input, train_output = generate_training_data(1)\n",
    "\n",
    "#Create and Initialize Network - Input size is 5, hidden layers are 4 nodes each and output size is 1\n",
    "W1,b1,W2,b2,W3,b3=create_network(5,4,4,1)\n",
    "\n",
    "prediction = forward_propagate(train_input[0],W1,b1,W2,b2,W3,b3)\n",
    "loss = calculate_mse_loss(prediction, train_output)\n",
    "\n",
    "print('Train input = ', train_input, 'Train_output = ',train_output, 'Prediction = ', prediction, 'Loss = ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All appears to be in order. The train output is indeed the sum of the input elements. The prediction is not accurate at present, but that is expected as the network has not been trained. The loss is square of the difference between the prediction and the train output, so that is working correctly. \n",
    "\n",
    "Now we need to calculate the gradients of the weights and biases so we can update the weights and improve the network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate gradients for weights and biases. We're going to use the 'rise/run' method to keep things mathematically simple, although it is certainly not going to be as fast as it could be.\n",
    "def calc_gradients(inputs,labels, W1,b1,W2,b2,W3,b3):\n",
    "    prediction = forward_propagate(inputs, W1,b1,W2,b2,W3,b3)\n",
    "    loss = calculate_mse_loss(prediction, labels)\n",
    "    # ~ print('Loss = ', loss)\n",
    "    #W1 gradient:\n",
    "    grad_W1 = np.zeros((np.shape(W1)))\n",
    "    for a in range(np.shape(grad_W1)[0]):\n",
    "        for b in range(np.shape(grad_W1)[1]):\n",
    "            temp_W1 = np.copy(W1)\n",
    "            temp_W1[a][b] += 0.01\n",
    "            temp_pred = forward_propagate(inputs,temp_W1,b1,W2,b2,W3,b3)\n",
    "            temp_loss = calculate_mse_loss(temp_pred, labels)\n",
    "            grad_W1[a][b] = (temp_loss-loss)/0.01\n",
    "    #b1 gradient:\n",
    "    grad_b1 = np.zeros((np.shape(b1)))\n",
    "    for a in range(np.shape(grad_b1)[0]):\n",
    "        temp_b1 = np.copy(b1)\n",
    "        temp_b1[a] += 0.01\n",
    "        temp_pred = forward_propagate(inputs,W1,temp_b1,W2,b2,W3,b3)\n",
    "        temp_loss = calculate_mse_loss(temp_pred, labels)\n",
    "        grad_b1[a] = (temp_loss-loss)/0.01\n",
    "   #W2 gradient:\n",
    "    grad_W2 = np.zeros((np.shape(W2)))\n",
    "    for a in range(np.shape(grad_W2)[0]):\n",
    "        for b in range(np.shape(grad_W2)[1]):\n",
    "            temp_W2 = np.copy(W2)\n",
    "            temp_W2[a][b] += 0.01\n",
    "            temp_pred = forward_propagate(inputs,W1,b1,temp_W2,b2,W3,b3)\n",
    "            temp_loss = calculate_mse_loss(temp_pred, labels)\n",
    "            grad_W2[a][b] = (temp_loss-loss)/0.01\n",
    "    #b1 gradient:\n",
    "    grad_b2 = np.zeros((np.shape(b2)))\n",
    "    for a in range(np.shape(grad_b2)[0]):\n",
    "        temp_b2 = np.copy(b2)\n",
    "        temp_b2[a] += 0.01\n",
    "        temp_pred = forward_propagate(inputs,W1,b1,W2,temp_b2,W3,b3)\n",
    "        temp_loss = calculate_mse_loss(temp_pred, labels)\n",
    "        grad_b2[a] = (temp_loss-loss)/0.01\n",
    "   #W1 gradient:\n",
    "    grad_W3 = np.zeros((np.shape(W3)))\n",
    "    for a in range(np.shape(grad_W3)[0]):\n",
    "        for b in range(np.shape(grad_W3)[1]):\n",
    "            temp_W3 = np.copy(W3)\n",
    "            temp_W3[a][b] += 0.01\n",
    "            temp_pred = forward_propagate(inputs,W1,b1,W2,b2,temp_W3,b3)\n",
    "            temp_loss = calculate_mse_loss(temp_pred, labels)\n",
    "            grad_W3[a][b] = (temp_loss-loss)/0.01\n",
    "    #b1 gradient:\n",
    "    grad_b3 = np.zeros((np.shape(b3)))\n",
    "    for a in range(np.shape(grad_b3)[0]):\n",
    "        temp_b3 = np.copy(b3)\n",
    "        temp_b3[a] += 0.0001\n",
    "        temp_pred = forward_propagate(inputs,W1,b1,W2,b2,W3,temp_b3)\n",
    "        temp_loss = calculate_mse_loss(temp_pred, labels)\n",
    "        grad_b3[a] = (temp_loss-loss)/0.0001\n",
    "    return(grad_W1,grad_b1,grad_W2,grad_b2,grad_W3,grad_b3,loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the biases, we need a way of updating the weights and biases based on these figures. We will add (or subtract, in the event of a negative gradient) the gradient multiplied by the learning rate from each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_weights(grad_W1,grad_b1,grad_W2,grad_b2,grad_W3,grad_b3, W1,b1,W2,b2,W3,b3,LR=0.001):\n",
    "    grads = [grad_W1,grad_b1,grad_W2,grad_b2,grad_W3,grad_b3]\n",
    "    weights = [W1,b1,W2,b2,W3,b3]\n",
    "    updated_weights = []\n",
    "    \n",
    "    for i in range(len(grads)):\n",
    "        updated_weights.append(weights[i] - LR*grads[i])\n",
    "    W1, b1, W2, b2, W3, b3 = updated_weights[0:6]\n",
    "    return(W1,b1,W2,b2,W3,b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put it all together to run an epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trains network over all training examples passed to it\n",
    "def train_epoch(inputs, outputs, W1,b1,W2,b2,W3,b3, LR=0.001):\n",
    "    loss_array = [] #So we can calculate the average loss over the epoch\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "    \n",
    "        grad_W1,grad_b1,grad_W2,grad_b2,grad_W3,grad_b3,loss = calc_gradients(inputs[i],outputs[i], W1,b1,W2,b2,W3,b3)\n",
    "        W1,b1,W2,b2,W3,b3 = update_weights(grad_W1,grad_b1,grad_W2,grad_b2,grad_W3,grad_b3, W1,b1,W2,b2,W3,b3,LR)\n",
    "        loss_array.append(loss)\n",
    "    \n",
    "    average_loss = np.average(loss_array)\n",
    "    \n",
    "    return(W1,b1,W2,b2,W3,b3,average_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything complete. A way to calculate the output of the network, to calculate the loss and the gradients, a way to update the weights and biases based on the gradients, and to repeat this over all elements in a training set. \n",
    "\n",
    "Let's now test the network to see if it works:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Average Loss =  4.84202027787\n",
      "Iteration 2 Average Loss =  2.72612892029\n",
      "Iteration 3 Average Loss =  1.90244051062\n",
      "Iteration 4 Average Loss =  1.37024288937\n",
      "Iteration 5 Average Loss =  1.01463595841\n",
      "Iteration 6 Average Loss =  0.771248255931\n",
      "Iteration 7 Average Loss =  0.601586531267\n",
      "Iteration 8 Average Loss =  0.481555952391\n",
      "Iteration 9 Average Loss =  0.395572901667\n",
      "Iteration 10 Average Loss =  0.333315056903\n",
      "Iteration 11 Average Loss =  0.287822472307\n",
      "Iteration 12 Average Loss =  0.254338425036\n",
      "Iteration 13 Average Loss =  0.229577717575\n",
      "Iteration 14 Average Loss =  0.211252881393\n",
      "Iteration 15 Average Loss =  0.19776146897\n",
      "Iteration 16 Average Loss =  0.18797683016\n"
     ]
    }
   ],
   "source": [
    "#Create and Initialize Network - Input size is 5, hidden layers are 4 nodes each and output size is 1\n",
    "W1,b1,W2,b2,W3,b3=create_network(5,4,4,1)\n",
    "\n",
    "#Generate Training Data - 1000 examples\n",
    "train_inputs, train_outputs = generate_training_data(1000)\n",
    "\n",
    "#Run for 20 epochs\n",
    "\n",
    "for i in range(20):    \n",
    "    W1,b1,W2,b2,W3,b3,average_loss = train_epoch(train_inputs,train_outputs,W1,b1,W2,b2,W3,b3, 1e-5)\n",
    "    print('Iteration',i+1, 'Average Loss = ', average_loss)\n",
    "\n",
    "#Test Performance\n",
    "print('TESTING:')\n",
    "print('Answer should be approx. 15', forward_propagate([1,2,3,4,5], W1,b1,W2,b2,W3,b3))\n",
    "print('Answer should be approx. 10', forward_propagate([2,2,2,2,2], W1,b1,W2,b2,W3,b3))\n",
    "print('Answer should be approx. 35', forward_propagate([9,8,7,6,5], W1,b1,W2,b2,W3,b3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It wprks! A neural network has been built from scratch. Performance could surely be improved by optimizing learning rate and/or running for a greater number of epochs, but the principle has been proven. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
